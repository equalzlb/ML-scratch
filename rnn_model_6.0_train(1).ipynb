{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<impala.hiveserver2.HiveServer2Connection object at 0x7f5f076bfcc0>\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from integrate_data_final import*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arguments {'train_steps': 409600, 'epoch': 10, 'batch_size': 8, 'evaluations': 1, 'logs_per_training': 20, 'eval_steps': 1, 'time_steps': 15, 'output_size': 5, 'embedding_size': 20, 'sparse_dim': 20, 'l2': None, 'learning_rate': 0.0002, 'num_rnn_nodes': 12, 'num_rnn_layers': 4, 'keep_prob': 1, 'feature_dim': 221, 'target_dim': 1, 'model_dir': './model/model_01'}\n",
      "INFO:tensorflow:Using config: {'_model_dir': './model/model_01', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 2560.0, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f5f05f8fb00>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Not using Distribute Coordinator.\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.\n",
      "WARNING:tensorflow:From /home/zhaolb/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "-------- Mode: TRAIN -----------\n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-2-803d94a15579>:116: GRUCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.GRUCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-2-803d94a15579>:122: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Value passed to parameter 'indices' has DataType float32 not in list of allowed values: int32, int64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mgather\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3270\u001b[0m     \u001b[0;31m# introducing a circular dependency.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3271\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3272\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'sparse_read'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-803d94a15579>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_verbosity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINFO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/platform/app.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, argv)\u001b[0m\n\u001b[1;32m    123\u001b[0m   \u001b[0;31m# Call the main function, passing through any arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m   \u001b[0;31m# to the final program.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m   \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-803d94a15579>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(argv)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Arguments'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     \u001b[0mestimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_evaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m     \u001b[0mmake_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-803d94a15579>\u001b[0m in \u001b[0;36mtrain_and_evaluate_model\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    263\u001b[0m     eval_spec = tf.estimator.EvalSpec(input_fn=lambda:input_vali_fn(args['batch_size'],args['feature_dim']),\n\u001b[1;32m    264\u001b[0m                                      steps=args['eval_steps'])\n\u001b[0;32m--> 265\u001b[0;31m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/training.py\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(estimator, train_spec, eval_spec)\u001b[0m\n\u001b[1;32m    469\u001b[0m         '(with task id 0).  Given task id {}'.format(config.task_id))\n\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/training.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    609\u001b[0m         config.task_type != run_config_lib.TaskType.EVALUATOR):\n\u001b[1;32m    610\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Running training and evaluation locally (non-distributed).'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_local\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m     \u001b[0;31m# Distributed case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/training.py\u001b[0m in \u001b[0;36mrun_local\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    710\u001b[0m         \u001b[0mmax_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m         \u001b[0mhooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_hooks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 712\u001b[0;31m         saving_listeners=saving_listeners)\n\u001b[0m\u001b[1;32m    713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m     eval_result = listener_for_eval.eval_result or _EvalResult(\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m       \u001b[0msaving_listeners\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_listeners_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss for final step: %s.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_distributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model_default\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1152\u001b[0m       \u001b[0mworker_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m       estimator_spec = self._call_model_fn(\n\u001b[0;32m-> 1154\u001b[0;31m           features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)\n\u001b[0m\u001b[1;32m   1155\u001b[0m       \u001b[0mglobal_step_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_global_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m       return self._train_with_estimator_spec(estimator_spec, worker_hooks,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_call_model_fn\u001b[0;34m(self, features, labels, mode, config)\u001b[0m\n\u001b[1;32m   1110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Calling model_fn.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1112\u001b[0;31m     \u001b[0mmodel_fn_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1113\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Done calling model_fn.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-803d94a15579>\u001b[0m in \u001b[0;36mrnn_model_fn\u001b[0;34m(features, labels, mode, params)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     _, encoder_state = encoding_layer(input_data,input_size,num_rnn_nodes,num_rnn_layers,\n\u001b[0;32m--> 201\u001b[0;31m                                       keep_prob,embedding_size,sparse_dim)\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m     training_decoder_output, inference_decoder_output = decoding_layer(batch_size,num_rnn_nodes,\n",
      "\u001b[0;32m<ipython-input-2-803d94a15579>\u001b[0m in \u001b[0;36mencoding_layer\u001b[0;34m(input_data, input_size, num_rnn_nodes, num_rnn_layers, keep_prob, embedding_size, sparse_dim)\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0minput_data_dense\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msparse_dim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0minput_data_sparse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msparse_dim\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0membedded_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0membedding_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msparse_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m     \u001b[0minput_data_concat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0membedded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_data_dense\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     encoder_output,encoder_state = tf.nn.dynamic_rnn(\n",
      "\u001b[0;32m<ipython-input-2-803d94a15579>\u001b[0m in \u001b[0;36membedding_layer\u001b[0;34m(input_data_sparse, embedding_size, sparse_dim)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'embedding_matix'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_data_sparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0membedding_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0membedded_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_lookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_data_sparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0membedded_input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/ops/embedding_ops.py\u001b[0m in \u001b[0;36membedding_lookup\u001b[0;34m(params, ids, partition_strategy, name, validate_indices, max_norm)\u001b[0m\n\u001b[1;32m    314\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m       \u001b[0mmax_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m       transform_fn=None)\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/ops/embedding_ops.py\u001b[0m in \u001b[0;36m_embedding_lookup_and_transform\u001b[0;34m(params, ids, partition_strategy, name, max_norm, transform_fn)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mtransform_fn\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         result = _clip(array_ops.gather(params[0], ids, name=name),\n\u001b[0m\u001b[1;32m    134\u001b[0m                        ids, max_norm)\n\u001b[1;32m    135\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtransform_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mgather\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3271\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3272\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3273\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mgather_v2\u001b[0;34m(params, indices, axis, name)\u001b[0m\n\u001b[1;32m   3746\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3747\u001b[0m   _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m-> 3748\u001b[0;31m         \"GatherV2\", params=params, indices=indices, axis=axis, name=name)\n\u001b[0m\u001b[1;32m   3749\u001b[0m   \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3750\u001b[0m   \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    608\u001b[0m               _SatisfiesTypeConstraint(base_type,\n\u001b[1;32m    609\u001b[0m                                        \u001b[0m_Attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m                                        param_name=input_name)\n\u001b[0m\u001b[1;32m    611\u001b[0m             \u001b[0mattrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m             \u001b[0minferred_from\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_SatisfiesTypeConstraint\u001b[0;34m(dtype, attr_def, param_name)\u001b[0m\n\u001b[1;32m     58\u001b[0m           \u001b[0;34m\"allowed values: %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m           (param_name, dtypes.as_dtype(dtype).name,\n\u001b[0;32m---> 60\u001b[0;31m            \", \".join(dtypes.as_dtype(x).name for x in allowed_list)))\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Value passed to parameter 'indices' has DataType float32 not in list of allowed values: int32, int64"
     ]
    }
   ],
   "source": [
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "np.set_printoptions(threshold=np.nan, linewidth=1000, precision=4)\n",
    "\n",
    "args = {'train_steps':409600,\n",
    "        'epoch':10,\n",
    "        'batch_size':8,\n",
    "        'evaluations':1,\n",
    "        'logs_per_training':20,\n",
    "        'eval_steps':1,\n",
    "        'time_steps':15,\n",
    "        'output_size':5,\n",
    "        'embedding_size':20,\n",
    "        'sparse_dim':20,\n",
    "        'l2':None,\n",
    "        'learning_rate':0.0002,\n",
    "        'num_rnn_nodes':12,\n",
    "        'num_rnn_layers':4,\n",
    "        'keep_prob':1,\n",
    "        'feature_dim':221,\n",
    "        'target_dim':1,\n",
    "        'model_dir':'./model/model_01'}\n",
    "\n",
    "\n",
    "\n",
    "                                                                                                                                      \n",
    "def input_train_fn(batch_size,epoch,feature_dim):\n",
    "#     data_encode = seq_encoder_train.astype(np.float32)\n",
    "#     data_decode = seq_decoder_train.astype(np.float32)\n",
    "    data_encode = np.random.rand(2560,10,feature_dim).astype(np.float32)\n",
    "    data_decode = np.random.rand(2560,5,1).astype(np.float32)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(({'encode':data_encode},{'decode':data_decode}))\n",
    "    dataset = dataset.batch(batch_size,drop_remainder=True)\n",
    "    dataset = dataset.repeat(epoch)\n",
    "    dateset = dataset.shuffle(1000)\n",
    "    return dataset\n",
    "\n",
    "def input_vali_fn(batch_size, feature_dim):\n",
    "    data_encode = np.random.rand(256,10,feature_dim).astype(np.float32)\n",
    "    data_decode = np.random.rand(256,5,1).astype(np.float32)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(({'encode':data_encode},{'decode':data_decode}))\n",
    "    dataset = dataset.batch(batch_size,drop_remainder=True)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def input_test_fn(batch_size, feature_dim):\n",
    "\n",
    "\n",
    "    test_data_encode = np.random.rand(256,10,feature_dim).astype(np.float32)\n",
    "    test_data_decode = np.random.rand(256,5,1).astype(np.float32)\n",
    "#     test_data_encode = build_seq_decoder_data_volume_ratio().astype(np.float32)\n",
    "#     test_data_decode = build_seq_decoder_data_volume_ratio().astype(np.float32)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(({'encode':test_data_encode},{'decode':test_data_decode}))\n",
    "    dataset = dataset.batch(batch_size,drop_remainder=True)\n",
    "    return dataset\n",
    "\n",
    "def pre_input_fn(batch_size,feature_dim,target_dim):   \n",
    "#     pre_data_encode = np.random.rand(batch_size,10,feature_dim).astype(np.float32)\n",
    "#     pre_data_decode = np.random.rand(batch_size,5,target_dim).astype(np.float32)\n",
    "    pre_data_encode = build_seq_decoder_data_volume_ratio().astype(np.float32)\n",
    "    pre_data_decode = build_seq_decoder_data_volume_ratio().astype(np.float32)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(({'encode':pre_data_encode},{'decode':pre_data_decode}))\n",
    "    dataset = dataset.batch(batch_size,drop_remainder=True) \n",
    "    return dataset\n",
    "\n",
    "def _reshape_input(data, batch_size, input_size, embedding_size):\n",
    "        \n",
    "    if data.get_shape().as_list()[1] != input_size:\n",
    "            raise AssertionError('The data does input_size %d instead of the expected %s'%\n",
    "                                 (data.get_shape().as_list()[1],input_size))\n",
    "        \n",
    "    if data.get_shape().as_list()[2] != embedding_size:\n",
    "            raise AssertionError('The data does feature_size %d instead of the expected %s'%\n",
    "                                (data.get_shape().as_list()[2],embedding_size))\n",
    "    reshaped = tf.reshape(data,[batch_size, -1, embedding_size])\n",
    "    \n",
    "def _get_batch_size(input_data):\n",
    "        \n",
    "    batch_size = input_data.get_shape().as_list()[0]\n",
    "    \n",
    "    if batch_size is None:\n",
    "            raise AssertionError('Batch size is not known')\n",
    "        \n",
    "    return batch_size\n",
    "\n",
    "def _loss_weights(batch_size):\n",
    "    loss_weights = tf.tile([0.5,0.15,0.15,0.1,0.1],[batch_size])\n",
    "    loss_weights = tf.reshape(loss_weights,[batch_size,5,1])\n",
    "    \n",
    "    return loss_weights\n",
    "\n",
    "def embedding_layer(input_data_sparse,embedding_size,sparse_dim):\n",
    "    \n",
    "    embedding = tf.get_variable('embedding_matix',[input_data_sparse.get_shape().as_list()[0],embedding_size])\n",
    "    embedded_input = tf.nn.embedding_lookup(embedding,input_data_sparse)\n",
    "    \n",
    "    return embedded_input\n",
    "\n",
    "\n",
    "    \n",
    "def _prepend_go_token(output_data, go_token, dim):\n",
    "    \n",
    "#     go_tokens = tf.fill([_get_batch_size(output_data),1,feature_dim],go_token)\n",
    "#     go_tokens = tf.cast(go_tokens,tf.float32)\n",
    "        \n",
    "    go_tokens = tf.constant(go_token, shape = [_get_batch_size(output_data),1, dim],dtype=tf.float32)\n",
    "    \n",
    "#     go_tokens = tf.tile(go_tokens, _get_batch_size((output_data),1,1))\n",
    "        \n",
    "    return tf.concat([go_tokens,output_data], axis=1)\n",
    "\n",
    "def _make_cell(rnn_size,keep_prob):\n",
    "    \n",
    "    \n",
    "    cell= tf.contrib.rnn.GRUCell(rnn_size,\n",
    "                                 kernel_initializer=tf.glorot_normal_initializer(),\n",
    "                                 activation=tf.nn.tanh)\n",
    "    dropout_cell = tf.contrib.rnn.DropoutWrapper(cell,output_keep_prob=keep_prob)\n",
    "    \n",
    "    return dropout_cell\n",
    "\n",
    "def encoding_layer(input_data, input_size, num_rnn_nodes, num_rnn_layers, keep_prob ,embedding_size,sparse_dim):\n",
    "    encoder_cell = tf.contrib.rnn.MultiRNNCell([_make_cell(num_rnn_nodes,keep_prob) for _ in range(num_rnn_layers)])\n",
    "    \n",
    "    input_data_dense = input_data[:,:,sparse_dim:]\n",
    "    input_data_sparse = input_data[:,:,:sparse_dim]\n",
    "    embedded_input = embedding_layer(input_data_sparse,embedding_size,sparse_dim)\n",
    "    input_data_concat = tf.concat([embedded_input, input_data_dense],axis=2)\n",
    "    encoder_output,encoder_state = tf.nn.dynamic_rnn(\n",
    "                                        encoder_cell,input_data,\n",
    "                                        sequence_length=[input_size]*_get_batch_size(input_data),\n",
    "                                        dtype=tf.float32)\n",
    "    \n",
    "    return encoder_output, encoder_state\n",
    "\n",
    "def decoding_layer(batch_size, num_rnn_nodes, num_rnn_layers, output_size,\n",
    "                  encoder_state, output_data, target_dim, go_token, regularizer,keep_prob):\n",
    "    \n",
    "    decoder_cell = tf.contrib.rnn.MultiRNNCell([_make_cell(num_rnn_nodes,keep_prob) for _ in range(num_rnn_layers)])\n",
    "    \n",
    "    projection_layer = tf.layers.Dense(units=target_dim,\n",
    "                                      kernel_initializer=tf.glorot_normal_initializer(),\n",
    "                                      kernel_regularizer=regularizer)\n",
    "    \n",
    "    training_decoder_output = None\n",
    "    with tf.variable_scope('decode'):\n",
    "        if output_data is not None:\n",
    "            decoder_input = _prepend_go_token(output_data, go_token, target_dim)\n",
    "            \n",
    "            training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=decoder_input,\n",
    "                                                               sequence_length=[output_size]*batch_size)\n",
    "            training_decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cell,training_helper,\n",
    "                                                               encoder_state,projection_layer)\n",
    "            training_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "                                                                training_decoder, impute_finished=True,\n",
    "                                                                maximum_iterations=output_size)\n",
    "    with tf.variable_scope('decode',reuse = tf.AUTO_REUSE):\n",
    "        start_tokens = tf.constant(go_token, shape = [batch_size, target_dim])\n",
    "        inference_helper = tf.contrib.seq2seq.InferenceHelper(\n",
    "                                sample_fn=lambda outputs:outputs,\n",
    "                                sample_shape=[target_dim],\n",
    "                                sample_dtype=tf.float32,\n",
    "                                start_inputs=start_tokens,\n",
    "                                end_fn=lambda sample_ids:False)\n",
    "        inference_decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cell,inference_helper,\n",
    "                                                           encoder_state,projection_layer)\n",
    "        \n",
    "        inference_decoder_output, _ ,_ = tf.contrib.seq2seq.dynamic_decode(\n",
    "                                                    inference_decoder, impute_finished=True,\n",
    "                                                    maximum_iterations=output_size)\n",
    "        \n",
    "    return training_decoder_output, inference_decoder_output\n",
    "    \n",
    "def rnn_model_fn(features, labels, mode, params):\n",
    "    print('-------- Mode:',mode.upper(),'-----------')\n",
    "    input_size = params['input_size']\n",
    "    output_size = params['output_size']\n",
    "    batch_size = params['batch_size']\n",
    "    learning_rate = params['learning_rate']\n",
    "    l2_regularization = params['l2_regularization']\n",
    "    feature_dim = params['feature_dim']\n",
    "    target_dim = params['target_dim']\n",
    "    keep_prob = params['keep_prob']\n",
    "    embedding_size = params['embedding_size']\n",
    "    sparse_dim = params['sparse_dim']\n",
    "    if l2_regularization:\n",
    "        regularizer = tf.contrib.layers.l2_regularizer(scale=l2_regularization)\n",
    "    else:\n",
    "        regularizer = None\n",
    "    num_rnn_layers = params['num_rnn_layers']\n",
    "    num_rnn_nodes = params['num_rnn_nodes']\n",
    "\n",
    "    input_data = features['encode']\n",
    "\n",
    "    if mode != tf.estimator.ModeKeys.PREDICT:\n",
    "        output_data = labels['decode']\n",
    "    else:\n",
    "        output_data = None\n",
    "    go_token = -1.0\n",
    "\n",
    "    _, encoder_state = encoding_layer(input_data,input_size,num_rnn_nodes,num_rnn_layers,\n",
    "                                      keep_prob,embedding_size,sparse_dim)\n",
    "\n",
    "    training_decoder_output, inference_decoder_output = decoding_layer(batch_size,num_rnn_nodes,\n",
    "                                                                      num_rnn_layers,output_size,\n",
    "                                                                      encoder_state,output_data,\n",
    "                                                                      target_dim,go_token,\n",
    "                                                                      regularizer,keep_prob)\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        predictions = inference_decoder_output.rnn_output\n",
    "       \n",
    "        return tf.estimator.EstimatorSpec(mode=mode,predictions=predictions)\n",
    "\n",
    "\n",
    "    predictions = training_decoder_output.rnn_output\n",
    "\n",
    "#     #loss = tf.losses.mean_squared_error(labels=output_data, predictions=predictions,\n",
    "#                                         weights=_loss_weights(args['batch_size']))\n",
    "    loss = tf.losses.huber_loss(labels=output_data, predictions=predictions,\n",
    "                                        weights=_loss_weights(args['batch_size']),\n",
    "                                        delta=0.2)\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "        gradients = optimizer.compute_gradients(loss)\n",
    "        capped_gradients = [(tf.clip_by_value(grad, -5. , 5.), var) for grad, var in gradients if grad is not None]\n",
    "        train_op = optimizer.apply_gradients(capped_gradients, global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "    assert mode == tf.estimator.ModeKeys.EVAL\n",
    "\n",
    "    return tf.estimator.EstimatorSpec(mode=mode, loss=loss)\n",
    "\n",
    "def train_and_evaluate_model(args):\n",
    "\n",
    "    if args['train_steps'] % args['batch_size']:\n",
    "        raise ValueError(\n",
    "            'The number of train steps %d must be a multiple of batch zie %d'%\n",
    "                (args['train_steps'], args['batch_size']))\n",
    "\n",
    "    params = {'input_size':args['time_steps']-args['output_size'],\n",
    "              'output_size':args['output_size'],\n",
    "              'batch_size':args['batch_size'],\n",
    "              'l2_regularization':args['l2'],\n",
    "              'learning_rate':args['learning_rate'],\n",
    "              'num_rnn_nodes':args['num_rnn_nodes'],\n",
    "              'num_rnn_layers':args['num_rnn_layers'],\n",
    "              'feature_dim':args['feature_dim'],\n",
    "              'target_dim':args['target_dim'],\n",
    "              'keep_prob':args['keep_prob'],\n",
    "              'embedding_size':20,\n",
    "              'sparse_dim':20,\n",
    "             }\n",
    "    log_step_count_steps = max(1, args['train_steps']/args['batch_size']/\n",
    "                               args['evaluations']//args['logs_per_training'])\n",
    "    estimator = tf.estimator.Estimator(\n",
    "                    model_dir=args['model_dir'],\n",
    "                    model_fn=rnn_model_fn,\n",
    "                    params=params,\n",
    "                    config=tf.estimator.RunConfig(log_step_count_steps=log_step_count_steps))\n",
    "    train_spec = tf.estimator.TrainSpec(input_fn=lambda:input_train_fn(args['batch_size'],args['epoch'],args['feature_dim']),\n",
    "                                       max_steps=args['train_steps']//args['batch_size'])\n",
    "    eval_spec = tf.estimator.EvalSpec(input_fn=lambda:input_vali_fn(args['batch_size'],args['feature_dim']),\n",
    "                                     steps=args['eval_steps'])\n",
    "    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n",
    "    \n",
    "    return estimator\n",
    "\n",
    "def _calcu_score(array1, array2):\n",
    "    sum_score = 0\n",
    "    weight = [0.5,0.15,0.15,0.1,0.1]\n",
    "    for i in range(5):\n",
    "        score = weight[i]*(1-abs(array2[i]-array1[i])/array1[i])\n",
    "        sum_score = sum_score+score\n",
    "        \n",
    "    if sum_score>=0:\n",
    "        return sum_score\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def get_score(expected,predicted):\n",
    "    expected = np.transpose(expected,axes=[0,2,1])\n",
    "    predicted = np.transpose(predicted,axes=[0,2,1])\n",
    "    assert expected.shape == predicted.shape\n",
    "    [dim1,dim2,_] = expected.shape \n",
    "    score_volume = []\n",
    "    score_hold = []\n",
    "    for i in range(dim1):\n",
    "\n",
    "        score_volume.append(_calcu_score(expected[i,0],predicted[i,0]))\n",
    "#         score_hold.append(_calcu_score(expected[i,1],predicted[i,1]))\n",
    "    return  np.asarray(score_volume)\n",
    "    \n",
    "\n",
    "def make_predictions(args,estimator):\n",
    "    predict_results = list(estimator.predict(input_fn=lambda:input_test_fn(args['batch_size'],args['feature_dim'])))\n",
    "    predict_results = np.asarray(predict_results)\n",
    "    test_dataset = input_test_fn(args['batch_size'],args['feature_dim'])\n",
    "    test_iterator = test_dataset.make_one_shot_iterator()\n",
    "    next_element = test_iterator.get_next()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        try:\n",
    "            batch_index = 0\n",
    "            average_score = []\n",
    "            j=1\n",
    "            TA_score = []\n",
    "            SR_score = []\n",
    "            MA_score = []\n",
    "            CF_score = []\n",
    "            ZC_score = []\n",
    "            AP_score = []\n",
    "            \n",
    "            for i in predict_results:\n",
    "                test_data=sess.run(next_element)\n",
    "                batch_predict=predict_results[batch_index:batch_index+args['batch_size']]\n",
    "                #print('expected:',test_data[1]['decode'])\n",
    "                zip_1 = zip(test_data[1]['decode'].tolist(),\n",
    "                               predict_results[batch_index:batch_index+args['batch_size']].tolist())\n",
    "                zip_2 = zip(zip_1,get_score(test_data[1]['decode'],batch_predict).tolist())\n",
    "            \n",
    "                for k in list(zip_2):\n",
    "                    print (k) \n",
    "                    print ('******************')\n",
    "                \n",
    "                                      \n",
    "              \n",
    "                j=j+1\n",
    "                average_score.append(np.average(get_score(test_data[1]['decode'],batch_predict)))\n",
    "                batch_index = batch_index+args['batch_size']\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print('average score total',np.average(average_score))\n",
    "            print('TA average score',np.average(TA_score))\n",
    "            print('SR average score',np.average(SR_score))\n",
    "            print('MA average score',np.average(MA_score))\n",
    "            print('CF average score',np.average(CF_score))\n",
    "            print('ZC average score',np.average(ZC_score))\n",
    "            print('AP average score',np.average(AP_score))\n",
    "    return predict_results\n",
    "    \n",
    "def _mse(expected, predicted):\n",
    "    return ((np.asarray(expected)-np.asarray(predicted)**2)).mean()\n",
    "\n",
    "def main(argv):\n",
    "    \n",
    "    print('Arguments',args)\n",
    "    estimator = train_and_evaluate_model(args)\n",
    "    make_predictions(args,estimator)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "    tf.app.run(main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
